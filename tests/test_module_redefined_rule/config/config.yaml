samples: config/samples.tsv

units: config/units.tsv

# Optional group annotation table. Uncomment to use.
# In the table, there has to be a column "group",
# with one entry for each group occurring in the
# sample sheet above. Any additional columns may be
# added to provide metadata for groups (e.g. if each
# group is a patient or individual).
# The given data will be shown in the header of the reported
# variant call overview tables.
# groups: config/groups.tsv

# Optional BED file with target regions
# target_regions: "path/to/taget-regions.bed"

ref:
  # Number of chromosomes to consider for calling.
  # The first n entries of the FASTA will be considered.
  n_chromosomes: 25
  # Ensembl species name
  species: homo_sapiens
  # Ensembl release
  release: 111
  # Genome build
  build: GRCh38
  pangenome: 
    # if active, reads will be aligned to given pangenome instead of to the linear reference genome
    # Important: this is only supported for homo_sapiens so far
    activate: true
    # Pangenome source and version: Only 'hprc' and v1.1 are supported for now. 
    # see https://github.com/human-pangenomics/hpp_pangenome_resources/?tab=readme-ov-file#minigraph-cactus for details
    source: hprc
    version: v1.1
  # Optionally, instead of downloading the whole reference from Ensembl via the
  # parameters above, specify a specific chromosome below and uncomment the line.
  # This is usually only relevant for testing.
  # chromosome: 21

# Trimming will be applied if global primer sequences are
# provided or primer panels are set in samplesheet
primers:
  trimming:
    # path to fasta files containing primer sequences
    primers_fa1: ""
    primers_fa2: ""
    # optional primer file allowing to define primers per sample
    # overwrites primers_fa1 and primers_fa2
    # the tsv file requires three fields: panel, fa1 and fa2 (optional)
    tsv: ""
    # Mean insert size between the outer primer ends.
    # If 0 or not set the bowtie default value of 250 will be used
    library_length: 0

# Estimation of tumor mutational burden.
mutational_burden:
  activate: false
  # Plotting modes - hist (stratified histogram)
  # or curve (stratified curve)
  mode:
    - curve
    - hist
  # List of varlociraptor events (as they occur in the scenario)
  # to calculate mutational burden for. Burden will be calculated
  # considering the sum of the posterior probabilties of the
  # given events.
  events:
    - somatic_tumor_low
    - somatic_tumor_medium
    - somatic_tumor_high

# Quantify known mutational signatures (human only)
mutational_signatures:
  activate: false
  events:
    # select events (callsets, defined under calling/fdr-control/events) to evaluate
    - some_id

# Sets the minimum average coverage for each gene.
# Genes with lower average coverage will not be concidered in gene coverage datavzrd report
# If not present min_avg_coverage will be set to 0 rendering all genes.
gene_coverage:
  min_avg_coverage: 5

# printing of variants in interactive tables
report:
  # if stratification is deactivated, one report for all
  # samples will be created.
  activate: true
  max_read_depth: 250
  stratify:
    activate: false
    # select a sample sheet column for stratification
    by-column: condition

# optional: standardized mutation overview plots with maf:
# https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html
maf:
  activate: false
  # sample alias from scenario.yaml for tumor sample;
  # only necessary, if activate: true
  primary_alias: "tumor"
  # optional: sample alias from scenario.yaml for normal sample
  # control_alias: "normal"


calling:
  # Set to true to infer classical genotypes from Varlociraptor's VAF predictions
  infer_genotypes: false
  delly:
    activate: true
    # Set custom excluded regions for delly calling. If commented out predefined templates will be downloaded
    # from https://github.com/dellytools/delly/tree/main/excludeTemplates
    # exclude_regions:
  freebayes:
    activate: true
  # See https://varlociraptor.github.io/docs/calling/#generic-variant-calling
  scenario: config/scenario.yaml
  filter:
    # Filter candidate variants (this filter helps to keep the number of evaluated candidates small).
    # It should ideally generate a superset of all other filters defined below.
    # Annotation of candidate variants tries to be as fast as possible, only using VEP
    # default parameters. Comment out or remove if not needed or intended.
    # Similar to other filters below, it is also possible to specify aux files here
    # (see gene_list_filter below). By this, you can e.g. conduct a virtual panel analysis
    # while massively speeding up the varlociraptor based evaluation of the variants.
    # See https://github.com/vembrane/vembrane
    candidates: "ANN['IMPACT'] != 'LOW'"
    # Add any number of named filters here (replace myfilter with a reasonable name,
    # and add more below). Filtering happens with the vembrane filter language
    # (see https://github.com/vembrane/vembrane), and you can refer to any fields that
    # have been added by VEP during annotation (in the INFO/ANN field, see section
    # annotations/vep below).
    # Filters will be applied independenty, and can be referred in FDR control below
    # to generate calls for different events.
    # You can for example filter by ANN fields, ID or dbsnp annotations here.
    impact_filter: "ANN['IMPACT'] == 'HIGH'"
    # If you need to use vembrane command line options beyond the filter expression,
    # please use the filter sub-structure with 'expression:' for the filter expression
    # and "aux-files:" for additional files with IDs for filtering. This can for example
    # be used for filtering by gene lists (with a file containing one gene name per line).
    gene_list_filter:
      aux-files:
        super_interesting_genes: "config/super_interesting_genes.tsv"
      expression: "ANN['SYMBOL'] in AUX['super_interesting_genes']"
  fdr-control:
    threshold: 0.05
    # Denote FDR control mode.
    # The default (local-smart) is recommended because it provides the most realistic and intuitive results.
    # See see https://varlociraptor.github.io/docs/filtering for details.
    mode: local-smart
    # Retain artifacts upon FDR control (i.e. do not remove them from the callset,
    # see https://varlociraptor.github.io/docs/filtering)
    # Only applies in smart mode (local-smart or global-smart).
    retain-artifacts: false
    events:
      # Add any number of events here to filter for.
      # The id of each event can be chosen freely, but needs to contain
      # only alphanumerics and underscores
      # ("some_id" below is just an example and can be modified as needed).
      some_id:
        types: ["variants", "fusions"]
        # labels for the callset, displayed in the report. Will fall back to id if no labels specified
        labels:
          some-label: label text
          other-label: label text
        # optional subcategory for callset (e.g. "known variants", or "VUS"),
        # comment out or remove to disable
        subcategory: some subcategory
        # Natural language description of the callset obtained here.
        # This will occur in the report.
        desc: |
          Some description.
        varlociraptor:
          # Add varlociraptor events to aggregated over.
          # The probability for the union of these events is used for controlling
          # the FDR.
          - present
          - somatic_tumor_high
          - somatic_tumor_medium
        filter: # myfilter
          # Add any number of filters here.
          # A single filter can be set as string.
          # Multiple filters can be defined as list and will be concatenated by 'and'
          - impact_filter
          - gene_list_filter

# If calc_consensus_reads is activated duplicates will be merged
remove_duplicates:
  activate: true

# Experimental: calculate consensus reads from PCR duplicates.
# if 'remove_duplicates' is deactivate only overlapping pairs will be merged
calc_consensus_reads:
  activate: false

population:
  db:
    # Set to true to continuously update a population database (BCF) with each new sample.
    activate: false
    # Path to a BCF file to be used to collect variants across mutliple runs of the pipeline.
    # The path can be outside of the workdir. Snakemake will update the file with each new
    # sample that occurs.
    path: ...
    # Alias of sample to be included into database
    alias: tumor
    # False discovery rate threshold of variants to be considered.
    fdr: 0.05
    # Varlociraptor events to be used.
    events:
      - somatic_tumor_high
      - somatic_tumor_medium

annotations:
  vcfs:
    activate: true
    # annotate with known variants from ensembl
    known: resources/variation.vcf.gz
    # add more external VCFs as needed
    # cosmic: path/to/cosmic.vcf.gz
  dgidb:
    # annotate variants with drug interactions of affected genes
    activate: false
    # List of datasources for filtering dgidb entries
    # Available sources can be found on http://dgidb.org/api/v2/interaction_sources.json
    datasources:
      - DrugBank
  vep:
    # Here, you can add annotations that you want to use in filtering out candidate variants
    # before they are put into varlociraptor for proper variant calling.
    candidate_calls:
      params: --af_gnomade
      plugins: []
    # Consider removing --everything if VEP is slow for you (e.g. for WGS),
    # and think carefully about which annotations you need.
    final_calls:
      params: --everything --check_existing
      plugins:
        # Add any plugin from https://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html
        # Plugin args can be passed as well, e.g. "LoFtool,path/to/custom/scores.txt".
        - LoFtool
        - REVEL
        # For integration of SpliceAI preprocessed snv and indel scores (including index files) are required. 
        # Scores are available on https://basespace.illumina.com/s/otSPW8hnhaZR
        #- SpliceAI,snv=<path/to/spliceai_scores.raw.snv.hg38.vcf.gz,indel=<path/to/spliceai_scores.raw.indel.hg38.vcf.gz>
        # For annotation of AlphaMissense scores a tsv-file containing processed scores is required.
        # Scores are available on https://zenodo.org/records/10813168
        # A tabix index is required and can be created by calling `tabix -s 1 -b 2 -e 2 -f -S 1 AlphaMissense_hg38.tsv.gz`
        #- AlphaMissense,file=<path/to/AlphaMissense_hg38.tsv.gz>

# printing of variants in a table format (might be deprecated soon)
tables:
  activate: false
  output:
    # Uncomment and add VEP annotation fields to include (IMPACT, Consequence, Feature, SYMBOL, and HGVSp are always included)
    annotation_fields:
      - EXON
    event_prob: true
    observations: true
  generate_excel: true

params:
  cutadapt: ""
  picard:
    MarkDuplicates: "--VALIDATION_STRINGENCY LENIENT"
  gatk:
    BaseRecalibrator: ""
    applyBQSR: ""
  varlociraptor:
    # add extra arguments for varlociraptor call
    # For example, in case of panel data consider to omit certain bias estimations
    # which might be misleading because all reads of an amplicon have the same start
    # position, strand etc. (--omit-strand-bias, --omit-read-position-bias,
    # --omit-softclip-bias, --omit-read-orientation-bias).
    call: ""
    # Add extra arguments for varlociraptor preprocess. By default, we limit the depth to 200.
    # Increase this value for panel sequencing!
    preprocess: "--max-depth 200"
  delly: ""
  freebayes:
    min_alternate_fraction: 0.05 # Reduce for calling variants with lower VAFs
    extra: ""

# If activated preprocessed alignment properties can be applied to each sample individually.
# Paths to the alignment properties json files need to be set in a tsv file containing a property name and path.
# Alignment properties names will be taken from a customizable column in the sample sheet.
# If no property name is set for a sample or custom_alignment_properties is deactivated the alignment properties will be estimated estimated on the sample's read alignments.
custom_alignment_properties:
  activate: false
  column: "panel"
  tsv: "config/alignment_properties.tsv"
